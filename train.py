# -*- coding: utf-8 -*-
"""xception_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ePUMbVwVuoK3eFL0LJ4uMvlu2EdpMyBY
"""

from google.colab import files
import zipfile
import os

# Upload dataset
uploaded = files.upload()

# Extract dataset
for filename in uploaded.keys():
    if filename.endswith(".zip"):
        with zipfile.ZipFile(filename, 'r') as zip_ref:
            zip_ref.extractall()  # Extract in the current directory
        os.remove(filename)  # Remove zip file

print("Dataset extracted successfully!")

# Verify folder structure
os.system("ls -R dataset")  # List all files in dataset folder

import os
print("Root directory contents:", os.listdir())  # List everything in root

import os

# Check root directory
root_contents = os.listdir()
print("üìÇ Root directory contents:", root_contents)

# Ensure 'Dataset' folder exists
if "Dataset" in root_contents:
    dataset_contents = os.listdir("Dataset")
    print("üìÇ Contents of 'Dataset':", dataset_contents)

    # Check if 'Train' folder is present inside 'Dataset'
    if "Train" in dataset_contents:
        train_contents = os.listdir("Dataset/Train")
        print("üìÇ Contents of 'Dataset/Train':", train_contents)

        # Verify Real and Fake folders exist
        if "Real" in train_contents and "Fake" in train_contents:
            print("‚úÖ 'Real' and 'Fake' folders are present.")
        else:
            print("‚ö†Ô∏è 'Real' or 'Fake' folder is missing! Check dataset structure.")
    else:
        print("‚ö†Ô∏è 'Train' folder NOT found inside 'Dataset'! Check folder structure.")
else:
    print("‚ö†Ô∏è 'Dataset' folder NOT found! Check extracted folder name.")

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define dataset path (Update if necessary)
DATASET_PATH = "/content/Dataset/Train"  # Adjust if your dataset is in a different location
IMG_SIZE = 128
BATCH_SIZE = 32

# Data Augmentation
datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # 80% train, 20% validation

# Load Training Data
train_generator = datagen.flow_from_directory(
    DATASET_PATH,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training'
)

# Load Validation Data
val_generator = datagen.flow_from_directory(
    DATASET_PATH,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation'
)

print("‚úÖ Data Generators Ready! Training can start now.")



IMG_SIZE = 128
EPOCHS_INITIAL = 5
EPOCHS_FINE_TUNE = 5

base_model = Xception(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))
base_model.trainable = False

x = base_model.output  # Connect to base model's output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation='relu')(x)
x = Dense(64, activation='relu')(x)
output_layer = Dense(2, activation='softmax')(x)  # ‚úÖ Corrected output connection

model = Model(inputs=base_model.input, outputs=output_layer)  # ‚úÖ Now correctly structured

model.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])

checkpoint = ModelCheckpoint("xception_finetuned.h5", save_best_only=True, monitor="val_accuracy", mode="max")

history1 = model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS_INITIAL, callbacks=[checkpoint])

# Unfreeze last 10 layers for fine-tuning
for layer in base_model.layers[-10:]:
    layer.trainable = True

model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])

history2 = model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS_FINE_TUNE, callbacks=[checkpoint])

model.save("xception_final.h5")

import numpy as np
import tensorflow as tf
import cv2
import matplotlib.pyplot as plt
from google.colab import files
from keras.preprocessing import image

# Load trained model
model = tf.keras.models.load_model("xception_final.h5")

# Upload an image
uploaded = files.upload()
img_path = list(uploaded.keys())[0]  # Get uploaded file name

# Load and preprocess the image
img = image.load_img(img_path, target_size=(128, 128))  # Ensure this matches training size
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
img_array /= 255.0  # Normalize if model was trained on normalized images

# Display the image
plt.imshow(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.show()

# Predict
prediction = model.predict(img_array)
predicted_class = np.argmax(prediction)
class_labels = ["Real","Fake"]  # Adjust based on dataset

print(f"Prediction: {class_labels[predicted_class]}")
print(f"Softmax Probabilities: {prediction}")

from google.colab import drive
drive.mount('/content/drive')
model.save("/content/drive/My Drive/xception_final.h5")
